{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5404/1450039869.py:38: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  page_count = browser.find_element_by_xpath('/html/body/main/div[2]/div/a/span[2]')\n",
      "/tmp/ipykernel_5404/1450039869.py:44: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  page_tag = browser.find_element_by_xpath('/html/body/main/div[2]/div/a/span[2]').text.replace(\" \", \"_\")\n",
      "/tmp/ipykernel_5404/1450039869.py:47: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  browser.find_element_by_xpath('//*[text() = \"JSON\"]').click()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def scrape_data(data_path, driver_path, state='archived', headless=True):\n",
    "    \"\"\"\n",
    "    Refresh the json data folders of petition data (petition type dependent).\n",
    "    \n",
    "    Note: The website JSON files contain more attributes than the CSV files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state      : petition type ['archived'/'closed'/'rejected'/'open']\n",
    "    data_path  : location to save petition json data\n",
    "    driver_path: location of web driver (currently Mozilla)\n",
    "    \"\"\"\n",
    "    \n",
    "    # To prevent download dialog\n",
    "    options = Options()\n",
    "    options.set_preference('browser.download.folderList', 2) # custom location\n",
    "    options.set_preference('browser.download.manager.showWhenStarting', False)\n",
    "    options.set_preference('browser.download.dir', data_path)\n",
    "    options.set_preference('browser.helperApps.neverAsk.saveToDisk', 'text/csv')\n",
    "    \n",
    "    # Without pop-up browser window (faster)\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "      \n",
    "    service = Service(driver_path)\n",
    "\n",
    "    browser = webdriver.Firefox(service=service, options=options)\n",
    "\n",
    "    # Find number of pages\n",
    "    browser.get(\"https://petition.parliament.uk/archived/petitions\")\n",
    "    page_count = browser.find_element_by_xpath('/html/body/main/div[2]/div/a/span[2]')\n",
    "    n_pages = int(page_count.text.split(' ')[-1])\n",
    "    \n",
    "    for i in np.arange(0, n_pages + 1, 1):\n",
    "        browser.get(\"https://petition.parliament.uk/archived/petitions?page=\" + str(i) + \"&state=\" + state)\n",
    "\n",
    "        page_tag = browser.find_element_by_xpath('/html/body/main/div[2]/div/a/span[2]').text.replace(\" \", \"_\")\n",
    "\n",
    "        # Move to JSON page\n",
    "        browser.find_element_by_xpath('//*[text() = \"JSON\"]').click()\n",
    "\n",
    "        # Download JSON\n",
    "        data = requests.get(browser.current_url).json()\n",
    "\n",
    "        # Save json to file\n",
    "        with open(data_path + 'data_' + page_tag + '.json', 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "        if i == n_pages:\n",
    "            # Last page has to be saved differently as the button changes\n",
    "            data = requests.get(browser.current_url).json()\n",
    "            page_tag = str(n_pages) + \"of\" + str(n_pages)\n",
    "            # Save json to file\n",
    "            with open(data_path + 'data_' + page_tag + '.json', 'w') as f:\n",
    "                json.dump(data, f)\n",
    "    \n",
    "    browser.close()\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "n_pages = 563 # Check number of pages on archives\n",
    "state = 'archived' # other states available\n",
    "parent_dir = '/home/will/Datasets/'\n",
    "driver_path = '/home/will/Projects/GovPetitionsUK/gov_uk_petitions_analysis/geckodriver'\n",
    "\n",
    "# Setup folders\n",
    "directory = 'petitions_website/'\n",
    "data_path = os.path.join(parent_dir, directory)\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "directory = 'petitions_website/' + state + '/'\n",
    "data_path = os.path.join(parent_dir, directory)\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "# Scrape the data\n",
    "scrape_data(data_path, driver_path, state, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gov_uk_env",
   "language": "python",
   "name": "gov_uk_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

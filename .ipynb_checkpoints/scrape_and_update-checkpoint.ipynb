{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5404/3904583128.py:17: DeprecationWarning: firefox_profile has been deprecated, please use an Options object\n",
      "  profile = webdriver.FirefoxProfile()\n",
      "/tmp/ipykernel_5404/3904583128.py:23: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Firefox(profile,executable_path=driver_path)\n",
      "/tmp/ipykernel_5404/3904583128.py:23: DeprecationWarning: firefox_profile has been deprecated, please pass in an Options object\n",
      "  browser = webdriver.Firefox(profile,executable_path=driver_path)\n",
      "/tmp/ipykernel_5404/3904583128.py:28: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  page_tag = browser.find_element_by_xpath('/html/body/main/div/div/a/span[2]').text.replace(\" \", \"\")\n",
      "/tmp/ipykernel_5404/3904583128.py:31: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  browser.find_element_by_xpath('//*[text() = \"JSON\"]').click()\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def scrape_data(state, n_pages, data_path, driver_path):\n",
    "    \"\"\"\n",
    "    Refresh the json data folders of petition data (petition type dependent)\n",
    "    \n",
    "    state: petition type ['archived'/'closed'/'rejected'/'open']\n",
    "    n_pages: number of pages on the website e.g. 563\n",
    "    \"\"\"\n",
    "    \n",
    "    # To prevent download dialog\n",
    "    profile = webdriver.FirefoxProfile()\n",
    "    profile.set_preference('browser.download.folderList', 2) # custom location\n",
    "    profile.set_preference('browser.download.manager.showWhenStarting', False)\n",
    "    profile.set_preference('browser.download.dir', data_path)\n",
    "    profile.set_preference('browser.helperApps.neverAsk.saveToDisk', 'text/csv')\n",
    "\n",
    "    browser = webdriver.Firefox(profile,executable_path=driver_path)\n",
    "\n",
    "    for i in np.arange(0, n_pages + 1, 1):\n",
    "        browser.get(\"https://petition.parliament.uk/archived/petitions?page=\" + str(i) + \"&state=\" + state)\n",
    "\n",
    "        page_tag = browser.find_element_by_xpath('/html/body/main/div/div/a/span[2]').text.replace(\" \", \"\")\n",
    "\n",
    "        # Move to JSON page\n",
    "        browser.find_element_by_xpath('//*[text() = \"JSON\"]').click()\n",
    "\n",
    "        # Download JSON\n",
    "        data = requests.get(browser.current_url).json()\n",
    "\n",
    "        # Save json to file\n",
    "        with open(data_path + 'data_' + page_tag + '.json', 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "        if i == n_pages:\n",
    "            # Last page has to be saved differently as the button changes\n",
    "            data = requests.get(browser.current_url).json()\n",
    "            page_tag = str(n_pages) + \"of\" + str(n_pages)\n",
    "            # Save json to file\n",
    "            with open(data_path + 'data_' + page_tag + '.json', 'w') as f:\n",
    "                json.dump(data, f)\n",
    "    \n",
    "    browser.close()\n",
    "\n",
    "# Parameters\n",
    "n_pages = 563 # Check number of pages on archives\n",
    "state = 'archived' # other states available\n",
    "parent_dir = '/home/will/Datasets/'\n",
    "driver_path = '/home/will/Projects/GovPetitionsUK/gov_uk_petitions_analysis/geckodriver'\n",
    "\n",
    "# Setup folders\n",
    "directory = 'petitions_website/'\n",
    "data_path = os.path.join(parent_dir, directory)\n",
    "os.mkdir(data_path)\n",
    "directory = 'petitions_website/' + state + '/'\n",
    "data_path = os.path.join(parent_dir, directory)\n",
    "os.mkdir(data_path)\n",
    "\n",
    "# Scrape the data\n",
    "scrape_data(state, n_pages, data_path, driver_path)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_json_to_df\u001b[39m(data_file):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m read_file:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Convert json files into single csv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def convert_json_to_df(data_file):\n",
    "    with open(data_file, \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "\n",
    "    data = pd.json_normalize(data['data'])\n",
    "    return data\n",
    "\n",
    "def get_files_in_folders(base_path, types):\n",
    "    all_file_paths = []\n",
    "    for i in types:\n",
    "        files = os.listdir(base_path + i)\n",
    "        full_path_files = [base_path + i + '/' + file for file in files]\n",
    "        all_file_paths.extend(full_path_files)\n",
    "    return all_file_paths\n",
    "\n",
    "def extract_specific_attribute(full_file_paths, attributes):\n",
    "    list_of_dataframes = []\n",
    "    for data_file in full_file_paths:\n",
    "        df = convert_json_to_df(data_file)\n",
    "        columns_to_extract = df.columns[df.columns.isin(attributes)] # Only filter attribute if present\n",
    "        list_of_dataframes.append(df[columns_to_extract]) \n",
    "    return combine_and_tidy_dfs(list_of_dataframes)\n",
    "\n",
    "\n",
    "def combine_and_tidy_dfs(list_of_dataframes, indexer='attributes.created_at'):\n",
    "    df = pd.concat(list_of_dataframes)\n",
    "    if df.columns[df.columns.isin([indexer])].any():\n",
    "        df[indexer] = pd.to_datetime(df[indexer])\n",
    "        df = df.set_index(indexer)\n",
    "        df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of votes over the history of online petitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['archived','closed']\n",
    "base_path = '/home/william/Datasets/petitions_website/'\n",
    "attributes = ['id','attributes.signature_count', 'attributes.created_at']\n",
    "full_file_paths = get_files_in_folders(base_path, types)\n",
    "full_df = extract_specific_attribute(full_file_paths, attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Date')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2,1, figsize=(20,10), sharex=True)\n",
    "full_df['weekly_sum_of_signatures'] = full_df['attributes.signature_count'].rolling('7D', closed='left').sum()\n",
    "\n",
    "full_df[['weekly_sum_of_signatures']].plot(rot=45,alpha=0.5, ax=ax[0], color='r')\n",
    "full_df[['attributes.signature_count']].plot(rot=45,alpha=0.5, ax=ax[1], color='b')\n",
    "\n",
    "ax[1].set_ylabel('Number of votes')\n",
    "ax[0].set_ylabel('Number of votes')\n",
    "ax[1].set_xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['archived','closed']\n",
    "base_path = '/home/william/Datasets/petitions_website/'\n",
    "full_file_paths = get_files_in_folders(base_path, types)\n",
    "\n",
    "def list_all_attributes(full_file_paths):\n",
    "    list_of_attributes = []\n",
    "    for data_file in full_file_paths:\n",
    "        #print(data_file)\n",
    "        df = convert_json_to_df(data_file).dropna(how='all', axis=1)\n",
    "        #print(df.columns)\n",
    "        list_of_attributes.extend(list(df.columns))\n",
    "    return set(list_of_attributes)\n",
    "\n",
    "ls_attr = list_all_attributes(full_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attributes.action',\n",
       " 'attributes.additional_details',\n",
       " 'attributes.background',\n",
       " 'attributes.closed_at',\n",
       " 'attributes.committee_note',\n",
       " 'attributes.created_at',\n",
       " 'attributes.debate.debate_pack_url',\n",
       " 'attributes.debate.debated_on',\n",
       " 'attributes.debate.overview',\n",
       " 'attributes.debate.transcript_url',\n",
       " 'attributes.debate.video_url',\n",
       " 'attributes.debate_outcome_at',\n",
       " 'attributes.debate_threshold_reached_at',\n",
       " 'attributes.departments',\n",
       " 'attributes.government_response.created_at',\n",
       " 'attributes.government_response.details',\n",
       " 'attributes.government_response.responded_on',\n",
       " 'attributes.government_response.summary',\n",
       " 'attributes.government_response.updated_at',\n",
       " 'attributes.government_response_at',\n",
       " 'attributes.moderation_threshold_reached_at',\n",
       " 'attributes.opened_at',\n",
       " 'attributes.rejected_at',\n",
       " 'attributes.rejection.code',\n",
       " 'attributes.rejection.details',\n",
       " 'attributes.response_threshold_reached_at',\n",
       " 'attributes.scheduled_debate_date',\n",
       " 'attributes.signature_count',\n",
       " 'attributes.state',\n",
       " 'attributes.topics',\n",
       " 'attributes.updated_at',\n",
       " 'id',\n",
       " 'links.self',\n",
       " 'type'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of debated petitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = ['archived','closed']\n",
    "base_path = '/home/william/Datasets/petitions_website/'\n",
    "attributes = ['id','attributes.signature_count', 'attributes.created_at', 'attributes.scheduled_debate_date',  \n",
    "              'attributes.debate.debate_pack_url',\n",
    "              'attributes.debate.debated_on',\n",
    "              'attributes.debate.overview',\n",
    "              'attributes.debate.transcript_url',\n",
    "              'attributes.debate.video_url',\n",
    "              'attributes.debate_outcome_at',\n",
    "              'attributes.debate_threshold_reached_at']\n",
    "full_file_paths = get_files_in_folders(base_path, types)\n",
    "full_df = extract_specific_attribute(full_file_paths, attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30196"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total debated petitions since 2017 : 43\n",
      "id 30196\n",
      "attributes.signature_count 30196\n",
      "attributes.debate_threshold_reached_at 39\n",
      "attributes.scheduled_debate_date 43\n",
      "attributes.debate_outcome_at 60\n",
      "attributes.debate.debated_on 54\n",
      "attributes.debate.transcript_url 60\n",
      "attributes.debate.video_url 60\n",
      "attributes.debate.debate_pack_url 60\n",
      "attributes.debate.overview 60\n"
     ]
    }
   ],
   "source": [
    "print(\"Total debated petitions since 2017 :\", len(full_df['attributes.scheduled_debate_date'].dropna()))\n",
    "\n",
    "\"\"\"\n",
    "Except, the gov website has 71 parliamentary petitions debated since June 2020. Therefore this attribute isn't\n",
    "reliable.\n",
    "\n",
    "Try others...\n",
    "\"\"\"\n",
    "\n",
    "for col in full_df.columns:\n",
    "    print(col, len(full_df[col].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate no data is lost during preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = ['archived','closed']\n",
    "base_path = '/home/william/Datasets/petitions_website/'\n",
    "attributes = ['id','attributes.signature_count', 'attributes.created_at']\n",
    "full_file_paths = get_files_in_folders(base_path, types)\n",
    "len(full_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30196"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "attributes = ['attributes.scheduled_debate_date',  \n",
    "              'attributes.debate.debate_pack_url',\n",
    "              'attributes.debate.debated_on',\n",
    "              'attributes.debate.overview',\n",
    "              'attributes.debate.transcript_url',\n",
    "              'attributes.debate.video_url',\n",
    "              'attributes.debate_outcome_at',\n",
    "              'attributes.debate_threshold_reached_at']\n",
    "out_list = extract_specific_attribute(full_file_paths, attributes)\n",
    "len(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30450"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28150"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "563*50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gov_uk_env",
   "language": "python",
   "name": "gov_uk_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
